{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Diagnostic Checks for SBI üîç\n",
    "\n",
    "**Time:** 20 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "\n",
    "In this exercise, you'll learn how to validate your neural posterior estimator using diagnostic tools. These checks are crucial for building trust in your inference results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will be able to:\n",
    "\n",
    "1. ‚úÖ Perform prior predictive checks to validate your prior\n",
    "2. ‚úÖ Monitor neural network training with loss curves\n",
    "3. ‚úÖ Run posterior predictive checks to validate inference\n",
    "4. ‚úÖ Use simulation-based calibration (SBC) to test the inference method\n",
    "5. ‚úÖ Interpret diagnostic results and identify problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ The Story Continues...\n",
    "\n",
    "Your initial analysis of the wolf-deer populations impressed the environmental agency! However, before they implement costly interventions based on your predictions, the senior ecologist asks:\n",
    "\n",
    "*\"How do we know the neural network learned correctly? What if it's just memorizing patterns rather than understanding the true dynamics? These predictions will inform important decisions about hunting quotas and conservation efforts.\"*\n",
    "\n",
    "Excellent question! Let's verify our results with diagnostic tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Quick Inference Recap\n",
    "\n",
    "Let's quickly repeat the inference from Exercise 1 (with fewer simulations for speed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sbi import utils as utils\n",
    "from sbi.diagnostics import run_sbc\n",
    "from sbi.inference import simulate_for_sbi\n",
    "\n",
    "# Our simulator\n",
    "from simulators.lotka_volterra import (\n",
    "    create_lotka_volterra_prior,\n",
    "    generate_observed_data,\n",
    "    get_lv_summary_stats_names,\n",
    "    lotka_volterra_simulator,\n",
    ")\n",
    "\n",
    "# Import our utility functions\n",
    "from utils import plot_predictive_check, plot_training_diagnostics\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 16\n",
    "\n",
    "# Setup\n",
    "USE_AUTOCORRELATION = True\n",
    "prior = create_lotka_volterra_prior()\n",
    "observed_data, true_params = generate_observed_data(\n",
    "    use_autocorrelation=USE_AUTOCORRELATION\n",
    ")\n",
    "lotka_volterra_simulator = partial(\n",
    "    lotka_volterra_simulator, use_autocorrelation=USE_AUTOCORRELATION\n",
    ")\n",
    "num_workers: int = 5\n",
    "\n",
    "# Let's load the inference object from exercise 01\n",
    "with open(\n",
    "    f\"lv_inference_{'with_autocorrelation' if USE_AUTOCORRELATION else 'without_autocorrelation'}.pt\",\n",
    "    \"rb\",\n",
    ") as handle:\n",
    "    npe = pickle.load(handle)\n",
    "# And build a new posterior object\n",
    "posterior = npe.build_posterior(prior=prior)\n",
    "# Set default x to observed data\n",
    "posterior.set_default_x(observed_data)\n",
    "\n",
    "print(\"\\n‚úÖ Inference loaded! Now let's check if we can trust it...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prior Predictive Check\n",
    "\n",
    "**Key Question:** *Does our observed data fall within the range of data that the prior can generate?*\n",
    "\n",
    "Before looking at the posterior, we should check if our prior is reasonable:\n",
    "1. Sample parameters from the prior\n",
    "2. Simulate data with those parameters  \n",
    "3. Check if observed data falls within this distribution\n",
    "\n",
    "If the observed data is far outside the prior predictive distribution, we may need to reconsider our prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Diagnostic 1: Prior Predictive Check\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Checking if our observed data is consistent with the prior...\\n\")\n",
    "\n",
    "# Define the statistics names for the Lotka-Volterra model\n",
    "stat_names = get_lv_summary_stats_names(USE_AUTOCORRELATION)\n",
    "\n",
    "# Simulate data from the prior\n",
    "# theta, x = simulate_for_sbi(lotka_volterra_simulator, prior, num_simulations=10000, num_workers=num_workers)\n",
    "x = npe._x_roundwise[0]\n",
    "prior_data_limits: list[tuple[float, float]] = [\n",
    "    (x.min(dim=0).values[i], x.max(dim=0).values[i]) for i in range(x.shape[1])\n",
    "]\n",
    "\n",
    "# Run the prior predictive check using our reusable function\n",
    "plot_predictive_check(\n",
    "    x=x,\n",
    "    observed_data=observed_data,\n",
    "    stat_names=stat_names,\n",
    "    title=\"Prior Predictive Check: Is our observed data consistent with the prior?\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Neural Network Training Diagnostics\n",
    "\n",
    "**Key Question:** *Did the neural network converge during training?*\n",
    "\n",
    "We need to check if the neural density estimator was trained properly:\n",
    "- Training loss should decrease and stabilize\n",
    "- Validation loss should not increase (no overfitting)\n",
    "- Both loss curves should ideally converge to a plateaux\n",
    "\n",
    "The `NPE` trainer class saves training and validation loss as well as other statistics\n",
    "during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can just pass the npe object to our plotting function.\n",
    "plot_training_diagnostics(npe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Posterior Predictive Check\n",
    "\n",
    "**Key Question:** *If we simulate data using parameters from our posterior, does it look like our observed data?*\n",
    "\n",
    "This is the most intuitive diagnostic:\n",
    "1. Sample parameters from the posterior\n",
    "2. Simulate data with those parameters\n",
    "3. Compare simulated data to observations\n",
    "\n",
    "If the posterior is correct, simulated data should be consistent with observations,\n",
    "e.g., it should \"center around\" the observed data. Some fluctuations around the observed\n",
    "data is valid, e.g., this could be due to simulator noise and parameter uncertainty.\n",
    "However, when the observed data lies at the edges or outside of the posterior predictive\n",
    "distributions, this hints at a problem with the overall inference setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive checks: simulate data with parameters samples from the posterior.\n",
    "_, predictive_sims = simulate_for_sbi(\n",
    "    lotka_volterra_simulator, posterior, num_simulations=1000, num_workers=num_workers\n",
    ")\n",
    "\n",
    "# Visualize the posterior predictive distributions together with the observed data.\n",
    "plot_predictive_check(\n",
    "    predictive_sims,\n",
    "    observed_data,\n",
    "    stat_names=stat_names,\n",
    "    limits=prior_data_limits,\n",
    "    percentile_allowance=90,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Simulation-Based Calibration (SBC)\n",
    "\n",
    "**Key Question:** *Are our credible intervals properly calibrated?*\n",
    "\n",
    "SBC is a test that checks if our inference method is statistically well calibrated:\n",
    "- Generate many \"ground truth\" parameters from the prior\n",
    "- For each, simulate data and perform inference (many different posteriors)\n",
    "- Check if X% credible intervals contain the true value X% of the time\n",
    "- Do this by calculating the rank of a given ground truth parameter under the\n",
    "  corresponding posterior--the ranks across all SBC samples must be uniformly\n",
    "  distributed\n",
    "\n",
    "Importantly, the SBC method can detect whether an SBI method is systematically biased,\n",
    "e.g., systematically over or underestimates the position of the underlying posterior, or\n",
    "whether the posterior variances are overconfident (too narrow) or underconfident (too\n",
    "wide). This difference scenarios are revealed by the shape of the rank distribution.\n",
    "\n",
    "Note, the SBC method can be \"gamed\" by just using a very wide posterior estimate, e.g.,\n",
    "the prior itself. Therefore, it should always be used in conjunction with posterior\n",
    "predictive checks that show that the posterior estimate is actually able to predict the\n",
    "observed data.\n",
    "\n",
    "We'll use the built-in `run_sbc` function from the sbi package for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Diagnostic 4: Simulation-Based Calibration (SBC)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Testing if our credible intervals are properly calibrated...\")\n",
    "print(\"(Note: Using fewer tests for speed - in practice, use 100-1000)\\n\")\n",
    "\n",
    "# Run SBC using sbi's built-in function\n",
    "num_sbc_runs = 200  # Reduced for tutorial speed (use 200+ in practice)\n",
    "\n",
    "thetas, xs = simulate_for_sbi(\n",
    "    lotka_volterra_simulator,\n",
    "    prior,\n",
    "    num_simulations=num_sbc_runs,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "print(f\"Running {num_sbc_runs} SBC tests...\")\n",
    "ranks, dap_samples = run_sbc(\n",
    "    thetas,\n",
    "    xs,\n",
    "    posterior,\n",
    "    num_posterior_samples=1000,\n",
    "    reduce_fns=\"marginals\",\n",
    "    use_batched_sampling=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis import sbc_rank_plot\n",
    "from sbi.diagnostics import check_sbc\n",
    "\n",
    "# Perform statistical checks on the SBC results:\n",
    "# Kolmogorov-Smirnov test on uniformity,\n",
    "# c2st test of ranks vs uniform dist,\n",
    "# c2st test of data averaged posterior samples vs prior samples\n",
    "ks_pvals, c2st_ranks, c2st_daps = check_sbc(ranks, thetas, dap_samples).values()\n",
    "\n",
    "param_names = [\n",
    "    \"Œ± (deer birth)\",\n",
    "    \"Œ≤ (predation)\",\n",
    "    \"Œ¥ (wolf efficiency)\",\n",
    "    \"Œ≥ (wolf death)\",\n",
    "]\n",
    "\n",
    "sbc_rank_plot(\n",
    "    ranks,\n",
    "    1000,\n",
    "    num_bins=100,\n",
    "    parameter_labels=param_names,\n",
    "    plot_type=\"cdf\",\n",
    "    ranks_labels=[\"Lotka-Volterra\"],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### Why Diagnostics Matter\n",
    "\n",
    "1. **Neural networks can fail silently** - They might produce confident-looking but wrong results\n",
    "2. **Not all posteriors are created equal** - Some might be overconfident, others too uncertain\n",
    "3. **Trust but verify** - Always check your inference before making decisions\n",
    "\n",
    "### Your Diagnostic Toolkit\n",
    "\n",
    "| Diagnostic | What it checks | Red flag |\n",
    "|------------|---------------|----------|\n",
    "| Prior Predictive | Prior covers observations | Observations in extreme tails |\n",
    "| Training Diagnostics | Network convergence | Loss increasing or unstable |\n",
    "| Posterior Predictive | Can recreate observations | Observations outside CI |\n",
    "| SBC | Calibrated credible intervals | Non-uniform rank histograms |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "‚úÖ **Always run diagnostics** - Make it part of your workflow  \n",
    "‚úÖ **Start with the prior** - Bad prior ‚Üí bad posterior  \n",
    "‚úÖ **Document results** - Keep diagnostic reports with your analysis  \n",
    "‚úÖ **Iterate if needed** - Poor diagnostics ‚Üí adjust and retrain  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge: Stress Testing\n",
    "\n",
    "Try breaking the inference and seeing how diagnostics detect it:\n",
    "\n",
    "1. **Too few simulations**: Retrain with only 500 simulations\n",
    "2. **Wrong prior**: Use a prior that doesn't contain true parameters\n",
    "3. **Corrupted data**: Add extreme outliers to observations\n",
    "\n",
    "How do the diagnostics change? Which tests catch which problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for experimentation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "euroscipy-2025-sbi-tutorial (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
