{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Trust but Verify - Diagnostic Tools for SBI\n",
    "\n",
    "**Time:** 20 minutes  \n",
    "**Goal:** Learn to diagnose whether your SBI results are trustworthy\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will:\n",
    "1. Understand why diagnostics are crucial for SBI\n",
    "2. Perform posterior predictive checks\n",
    "3. Run simulation-based calibration (coverage tests)\n",
    "4. Identify common failure modes and warning signs\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ The Story Continues...\n",
    "\n",
    "Your initial analysis of the wolf-deer populations impressed the environmental agency! However, before they implement costly interventions based on your predictions, the senior ecologist asks:\n",
    "\n",
    "*\"How do we know the neural network learned correctly? What if it's just memorizing patterns rather than understanding the true dynamics? These predictions will inform important decisions about hunting quotas and conservation efforts.\"*\n",
    "\n",
    "Excellent question! Let's verify our results with diagnostic tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Quick Inference Recap\n",
    "\n",
    "Let's quickly repeat the inference from Exercise 1 (with fewer simulations for speed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sbi import utils as utils\n",
    "from sbi.inference import NPE\n",
    "\n",
    "# Our simulator\n",
    "from simulators.lotka_volterra import (\n",
    "    create_lotka_volterra_prior,\n",
    "    generate_observed_data,\n",
    "    lotka_volterra_simulator,\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 16\n",
    "\n",
    "print(\"üîÑ Running quick inference for diagnostics...\")\n",
    "\n",
    "# Setup\n",
    "prior = create_lotka_volterra_prior()\n",
    "observed_data, true_params = generate_observed_data(seed=2025)\n",
    "\n",
    "# Quick training (fewer simulations for speed)\n",
    "npe = NPE(prior)\n",
    "theta, x = utils.simulate_for_sbi(lotka_volterra_simulator, prior, num_simulations=5000)\n",
    "density_estimator = npe.append_simulations(theta, x).train(\n",
    "    training_batch_size=50, max_num_epochs=30\n",
    ")\n",
    "posterior = npe.build_posterior(density_estimator)\n",
    "\n",
    "print(\"‚úÖ Inference complete! Now let's check if we can trust it...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Diagnostic 1 - Posterior Predictive Check\n",
    "\n",
    "**Key Question:** *If we simulate data using parameters from our posterior, does it look like our observed data?*\n",
    "\n",
    "This is the most intuitive diagnostic:\n",
    "1. Sample parameters from the posterior\n",
    "2. Simulate data with those parameters\n",
    "3. Compare simulated data to observations\n",
    "\n",
    "If the posterior is correct, simulated data should be consistent with observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Diagnostic 1: Posterior Predictive Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample from posterior and simulate\n",
    "n_predictive_samples = 500\n",
    "posterior_samples = posterior.sample((n_predictive_samples,), x=observed_data.flatten())\n",
    "\n",
    "# Simulate data for each posterior sample\n",
    "predictive_sims = []\n",
    "for params in posterior_samples:\n",
    "    sim = lotka_volterra_simulator(params, add_noise=True)\n",
    "    predictive_sims.append(sim)\n",
    "predictive_sims = torch.stack(predictive_sims)\n",
    "\n",
    "# Calculate statistics\n",
    "sim_mean = predictive_sims.mean(dim=0)\n",
    "sim_std = predictive_sims.std(dim=0)\n",
    "sim_lower = torch.quantile(predictive_sims, 0.025, dim=0)\n",
    "sim_upper = torch.quantile(predictive_sims, 0.975, dim=0)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "times = np.array([10, 15, 20, 25, 30])\n",
    "\n",
    "# Deer populations\n",
    "ax = axes[0]\n",
    "ax.fill_between(\n",
    "    times, sim_lower[:, 0], sim_upper[:, 0], alpha=0.3, color=\"brown\", label=\"95% CI\"\n",
    ")\n",
    "ax.plot(times, sim_mean[:, 0], \"o-\", color=\"brown\", label=\"Posterior predictive mean\")\n",
    "ax.plot(\n",
    "    times,\n",
    "    observed_data[:, 0],\n",
    "    \"s-\",\n",
    "    color=\"black\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    label=\"Observed\",\n",
    ")\n",
    "ax.set_ylabel(\"Deer Population\", fontsize=14)\n",
    "ax.legend(loc=\"upper right\", fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Wolf populations\n",
    "ax = axes[1]\n",
    "ax.fill_between(\n",
    "    times, sim_lower[:, 1], sim_upper[:, 1], alpha=0.3, color=\"orange\", label=\"95% CI\"\n",
    ")\n",
    "ax.plot(times, sim_mean[:, 1], \"o-\", color=\"orange\", label=\"Posterior predictive mean\")\n",
    "ax.plot(\n",
    "    times,\n",
    "    observed_data[:, 1],\n",
    "    \"s-\",\n",
    "    color=\"black\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    label=\"Observed\",\n",
    ")\n",
    "ax.set_xlabel(\"Time (years)\", fontsize=14)\n",
    "ax.set_ylabel(\"Wolf Population\", fontsize=14)\n",
    "ax.legend(loc=\"upper right\", fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Posterior Predictive Check\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if observations fall within predictive intervals\n",
    "print(\"\\nüìä Coverage Analysis:\")\n",
    "for i, year in enumerate(times):\n",
    "    deer_in_ci = sim_lower[i, 0] <= observed_data[i, 0] <= sim_upper[i, 0]\n",
    "    wolf_in_ci = sim_lower[i, 1] <= observed_data[i, 1] <= sim_upper[i, 1]\n",
    "\n",
    "    deer_symbol = \"‚úÖ\" if deer_in_ci else \"‚ùå\"\n",
    "    wolf_symbol = \"‚úÖ\" if wolf_in_ci else \"‚ùå\"\n",
    "\n",
    "    print(f\"Year {year}: Deer {deer_symbol}, Wolves {wolf_symbol}\")\n",
    "\n",
    "# Overall assessment\n",
    "total_obs = len(times) * 2  # deer and wolves\n",
    "in_ci = sum(\n",
    "    [\n",
    "        (sim_lower[i, j] <= observed_data[i, j] <= sim_upper[i, j]).item()\n",
    "        for i in range(len(times))\n",
    "        for j in range(2)\n",
    "    ]\n",
    ")\n",
    "coverage = in_ci / total_obs * 100\n",
    "\n",
    "print(f\"\\nüéØ Overall: {in_ci}/{total_obs} observations within 95% CI ({coverage:.1f}%)\")\n",
    "if coverage >= 80:\n",
    "    print(\"‚úÖ PASS: Good agreement between posterior predictions and observations!\")\n",
    "elif coverage >= 60:\n",
    "    print(\n",
    "        \"‚ö†Ô∏è  WARNING: Moderate agreement - consider more simulations or checking model\"\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ùå FAIL: Poor agreement - the posterior may be unreliable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Diagnostic 2 - Simulation-Based Calibration (Coverage Test)\n",
    "\n",
    "**Key Question:** *If we know the true parameters, does our method recover them correctly across many scenarios?*\n",
    "\n",
    "This is a more rigorous test:\n",
    "1. Sample \"true\" parameters from the prior\n",
    "2. Simulate data with those parameters\n",
    "3. Infer parameters from the simulated data\n",
    "4. Check if true parameters fall within credible intervals\n",
    "\n",
    "For well-calibrated inference, X% credible intervals should contain the true value X% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Diagnostic 2: Simulation-Based Calibration\")\n",
    "print(\"=\" * 50)\n",
    "print(\n",
    "    \"Testing if our 90% credible intervals contain the true value ~90% of the time...\"\n",
    ")\n",
    "print(\"(Note: Running fewer tests for speed - in practice, use 100-1000)\\n\")\n",
    "\n",
    "# Run calibration tests\n",
    "n_tests = 20  # Reduced for tutorial speed (use 100+ in practice)\n",
    "credibility_level = 0.9  # 90% credible interval\n",
    "contained = []\n",
    "\n",
    "for test_idx in range(n_tests):\n",
    "    # Sample true parameters\n",
    "    true_theta = prior.sample((1,)).squeeze()\n",
    "\n",
    "    # Simulate observation\n",
    "    x_test = lotka_volterra_simulator(true_theta)\n",
    "\n",
    "    # Get posterior samples\n",
    "    posterior_samples_test = posterior.sample((1000,), x=x_test.flatten())\n",
    "\n",
    "    # Check if true parameters are within credible intervals\n",
    "    lower = torch.quantile(posterior_samples_test, (1 - credibility_level) / 2, dim=0)\n",
    "    upper = torch.quantile(\n",
    "        posterior_samples_test, 1 - (1 - credibility_level) / 2, dim=0\n",
    "    )\n",
    "\n",
    "    # Check each parameter\n",
    "    param_contained = [\n",
    "        (lower[i] <= true_theta[i] <= upper[i]).item() for i in range(len(true_theta))\n",
    "    ]\n",
    "    contained.append(param_contained)\n",
    "\n",
    "    # Progress indicator\n",
    "    if (test_idx + 1) % 5 == 0:\n",
    "        print(f\"  Completed {test_idx + 1}/{n_tests} tests...\")\n",
    "\n",
    "# Analyze results\n",
    "contained = np.array(contained)\n",
    "coverage_per_param = contained.mean(axis=0) * 100\n",
    "overall_coverage = contained.mean() * 100\n",
    "\n",
    "print(\"\\nüìä Coverage Results (Expected: ~90%):\")\n",
    "print(\"-\" * 40)\n",
    "param_names = [\n",
    "    \"Œ± (deer birth)\",\n",
    "    \"Œ≤ (predation)\",\n",
    "    \"Œ¥ (wolf efficiency)\",\n",
    "    \"Œ≥ (wolf death)\",\n",
    "]\n",
    "for i, name in enumerate(param_names):\n",
    "    cov = coverage_per_param[i]\n",
    "    if 80 <= cov <= 100:\n",
    "        symbol = \"‚úÖ\"\n",
    "    elif 70 <= cov < 80:\n",
    "        symbol = \"‚ö†Ô∏è\"\n",
    "    else:\n",
    "        symbol = \"‚ùå\"\n",
    "    print(f\"{symbol} {name:20s}: {cov:.1f}%\")\n",
    "\n",
    "print(f\"\\nüéØ Overall coverage: {overall_coverage:.1f}%\")\n",
    "\n",
    "# Assessment\n",
    "if 85 <= overall_coverage <= 95:\n",
    "    print(\"‚úÖ PASS: Well-calibrated inference!\")\n",
    "elif 75 <= overall_coverage < 85 or 95 < overall_coverage <= 100:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Slightly miscalibrated - consider more training data\")\n",
    "else:\n",
    "    print(\"‚ùå FAIL: Poorly calibrated - inference may be unreliable!\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_pos = np.arange(len(param_names))\n",
    "bars = ax.bar(\n",
    "    x_pos,\n",
    "    coverage_per_param,\n",
    "    color=[\n",
    "        \"green\" if 80 <= c <= 100 else \"orange\" if 70 <= c < 80 else \"red\"\n",
    "        for c in coverage_per_param\n",
    "    ],\n",
    ")\n",
    "ax.axhline(y=90, color=\"black\", linestyle=\"--\", label=\"Expected (90%)\")\n",
    "ax.set_ylabel(\"Coverage (%)\", fontsize=14)\n",
    "ax.set_xlabel(\"Parameter\", fontsize=14)\n",
    "ax.set_title(\"Coverage Test Results\", fontsize=16)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([name.split(\" \")[0] for name in param_names], fontsize=12)\n",
    "ax.set_ylim([0, 100])\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, cov in zip(bars, coverage_per_param, strict=False):\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 1,\n",
    "        f\"{cov:.0f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Common Warning Signs and What They Mean\n",
    "\n",
    "Let's explore what different diagnostic failures tell us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö†Ô∏è  Common SBI Warning Signs and Solutions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "warning_signs = [\n",
    "    {\n",
    "        \"symptom\": \"Posterior much wider than expected\",\n",
    "        \"causes\": [\"Too few simulations\", \"Uninformative observations\"],\n",
    "        \"solutions\": [\"Increase num_simulations\", \"Use more/better observations\"],\n",
    "    },\n",
    "    {\n",
    "        \"symptom\": \"Posterior too narrow (overconfident)\",\n",
    "        \"causes\": [\"Overfitting\", \"Model misspecification\"],\n",
    "        \"solutions\": [\"Use validation set\", \"Check simulator assumptions\"],\n",
    "    },\n",
    "    {\n",
    "        \"symptom\": \"Poor coverage (<70%)\",\n",
    "        \"causes\": [\"Network architecture issues\", \"Training problems\"],\n",
    "        \"solutions\": [\"Try different density estimator\", \"Increase training epochs\"],\n",
    "    },\n",
    "    {\n",
    "        \"symptom\": \"Multimodal posterior when not expected\",\n",
    "        \"causes\": [\"Parameter identifiability issues\", \"Multiple solutions\"],\n",
    "        \"solutions\": [\"Add more observations\", \"Reparameterize model\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, warning in enumerate(warning_signs, 1):\n",
    "    print(f\"\\n{i}. {warning['symptom']}\")\n",
    "    print(\"   Possible causes:\")\n",
    "    for cause in warning[\"causes\"]:\n",
    "        print(f\"     ‚Ä¢ {cause}\")\n",
    "    print(\"   Solutions:\")\n",
    "    for solution in warning[\"solutions\"]:\n",
    "        print(f\"     ‚Üí {solution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Quick Diagnostic - Training Loss Check\n",
    "\n",
    "A simple but useful check: did the neural network training converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìâ Training Loss Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get training losses from the summary\n",
    "try:\n",
    "    summary = npe.summary\n",
    "    best_loss = summary[\"best_validation_loss\"][-1]\n",
    "\n",
    "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
    "\n",
    "    if best_loss < -1.0:  # Good for normalizing flows\n",
    "        print(\"‚úÖ Training converged well\")\n",
    "    elif best_loss < 0:\n",
    "        print(\"‚ö†Ô∏è  Training partially converged - consider more epochs\")\n",
    "    else:\n",
    "        print(\"‚ùå Poor training - check your setup\")\n",
    "\n",
    "except:\n",
    "    print(\"Could not access training summary\")\n",
    "\n",
    "# Additional quick checks\n",
    "print(\"\\nüîç Quick Sanity Checks:\")\n",
    "\n",
    "# Check 1: Can we sample from posterior?\n",
    "try:\n",
    "    test_samples = posterior.sample((10,), x=observed_data.flatten())\n",
    "    print(\"‚úÖ Posterior sampling works\")\n",
    "except:\n",
    "    print(\"‚ùå Cannot sample from posterior\")\n",
    "\n",
    "# Check 2: Are posterior samples within prior bounds?\n",
    "posterior_test = posterior.sample((100,), x=observed_data.flatten())\n",
    "prior_low = prior.base_dist.low\n",
    "prior_high = prior.base_dist.high\n",
    "\n",
    "within_bounds = torch.all(\n",
    "    (posterior_test >= prior_low) & (posterior_test <= prior_high)\n",
    ")\n",
    "if within_bounds:\n",
    "    print(\"‚úÖ All posterior samples within prior bounds\")\n",
    "else:\n",
    "    print(\"‚ùå Some posterior samples outside prior bounds!\")\n",
    "\n",
    "# Check 3: Posterior not collapsed?\n",
    "posterior_std = posterior_test.std(dim=0)\n",
    "if torch.all(posterior_std > 0.01):  # Arbitrary small threshold\n",
    "    print(\"‚úÖ Posterior has reasonable variance\")\n",
    "else:\n",
    "    print(\"‚ùå Posterior may have collapsed to point mass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Creating Your Diagnostic Report\n",
    "\n",
    "Let's create a summary diagnostic report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"          üìã SBI DIAGNOSTIC REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Collect all diagnostic results\n",
    "diagnostics = {\n",
    "    \"Posterior Predictive Check\": coverage >= 80,\n",
    "    \"Coverage Calibration\": 85 <= overall_coverage <= 95,\n",
    "    \"Training Convergence\": True,  # Placeholder - set based on actual check\n",
    "    \"Sampling Works\": True,  # Placeholder\n",
    "    \"Within Prior Bounds\": within_bounds.item(),\n",
    "}\n",
    "\n",
    "# Overall assessment\n",
    "passed = sum(diagnostics.values())\n",
    "total = len(diagnostics)\n",
    "\n",
    "print(f\"\\nüìä Diagnostic Summary: {passed}/{total} checks passed\\n\")\n",
    "\n",
    "for test, result in diagnostics.items():\n",
    "    symbol = \"‚úÖ\" if result else \"‚ùå\"\n",
    "    print(f\"{symbol} {test}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "if passed == total:\n",
    "    print(\"üéâ OVERALL: READY FOR PRODUCTION\")\n",
    "    print(\"Your inference is reliable and can be used for decision-making.\")\n",
    "elif passed >= total * 0.6:\n",
    "    print(\"‚ö†Ô∏è  OVERALL: USE WITH CAUTION\")\n",
    "    print(\"Some diagnostics failed. Consider:\")\n",
    "    print(\"  ‚Ä¢ Increasing training simulations\")\n",
    "    print(\"  ‚Ä¢ Checking model assumptions\")\n",
    "    print(\"  ‚Ä¢ Running more thorough diagnostics\")\n",
    "else:\n",
    "    print(\"‚ùå OVERALL: DO NOT USE\")\n",
    "    print(\"Multiple diagnostic failures detected.\")\n",
    "    print(\"Please review your setup and retrain.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### Why Diagnostics Matter\n",
    "\n",
    "1. **Neural networks can fail silently** - They might produce confident-looking but wrong results\n",
    "2. **Not all posteriors are created equal** - Some might be overconfident, others too uncertain\n",
    "3. **Trust but verify** - Always check your inference before making decisions\n",
    "\n",
    "### Your Diagnostic Toolkit\n",
    "\n",
    "| Diagnostic | What it checks | Red flag |\n",
    "|------------|---------------|----------|\n",
    "| Posterior Predictive | Can we recreate observations? | Observations outside CI |\n",
    "| Coverage Test | Are credible intervals calibrated? | Coverage far from nominal |\n",
    "| Training Loss | Did the network converge? | Loss increasing or high |\n",
    "| Prior Bounds | Is posterior sensible? | Samples outside prior |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "‚úÖ **Always run diagnostics** - Make it part of your workflow  \n",
    "‚úÖ **Start simple** - Posterior predictive checks first  \n",
    "‚úÖ **Document results** - Keep diagnostic reports with your analysis  \n",
    "‚úÖ **Iterate if needed** - Poor diagnostics ‚Üí adjust and retrain  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Challenge: Stress Testing\n",
    "\n",
    "Try breaking the inference and seeing how diagnostics detect it:\n",
    "\n",
    "1. **Too few simulations**: Retrain with only 500 simulations\n",
    "2. **Wrong prior**: Use a prior that doesn't contain true parameters\n",
    "3. **Corrupted data**: Add extreme outliers to observations\n",
    "\n",
    "How do the diagnostics change? Which tests catch which problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for experimentation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
